{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "\n",
    "Use: attention mechanism, GRUCell\n",
    "\n",
    "- The encoder: Bidirectional RNN\n",
    "- What I should do: make encoder(bidirectional RNN using GRU) and decoder\n",
    "- 문장 -> 단어(tokenization) -> embedding -> enc-dec -> loss check -> embedding -> output\n",
    "\n",
    "This is not a complete code for this paper. This might occur error in many ways. But I try to show the process in paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import math\n",
    "from tensorflow.contrib import rnn\n",
    "import re\n",
    "\n",
    "#initialize\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Extra vocabulary symbols\n",
    "_GO = '_GO'\n",
    "_EOS = '_EOS'\n",
    "_UNK = '_UNK'\n",
    "\n",
    "extra_tokens = [_GO, _EOS, _UNK]\n",
    "\n",
    "start_token = extra_tokens.index(_GO)   # start = 0\n",
    "end_token = extra_tokens.index(_EOS)    # end = 1\n",
    "unk_token = extra_tokens.index(_UNK)\n",
    "\n",
    "_WORD_SPLIT = re.compile(r'[,.!?\"\\':;)(]')\n",
    "\n",
    "hidden_units = 1000\n",
    "embedding_size = 620\n",
    "max_decode_step = 500\n",
    "alignment_hidden_units = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "inputs = \"정부는 공공기관 주차장을 전면 폐쇄하고 관용차 3만3000여대 운행을 중단한다. 15일 첫 시행 당시 여러 허점을 노출한 터라 서울시 미세먼지 대책 실효성 논란은 더욱 커질 전망이다. 나는 개를 좋아한다. 나는 수박을 좋아한다. 나는 너를 사랑한다. 나는 컴퓨터를 사랑한다.\"\n",
    "outputs = \"The government will shut down parking lots at public institutions and shut down the operation of about 33000 private cars. The Seoul metropolitan government is expected to increase the controversy on the effectiveness of its countermeasures. I like dogs. I like watermelons. I love you. I love computers.\"\n",
    "\"\"\"\n",
    "inputs, outputs는 파일에서 읽어오는 방식으로 바꿔도 가능, str 형태면 됨\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "\n",
    "    result = [_GO] + [w for w in words if w] + [_EOS]\n",
    "    return result\n",
    "\n",
    "\n",
    "def make_data(inputs):\n",
    "    new_text = re.sub(_WORD_SPLIT, '', inputs)\n",
    "    new_text.replace('.', ' {0}'.format(_EOS))\n",
    "\n",
    "    words_list = list(set(new_text.split(\" \")))\n",
    "    num_encoder_symbols = len(words_list)\n",
    "    word2idx = {c: i + 3 for i, c in enumerate(words_list)}\n",
    "    word2idx[_EOS] = end_token\n",
    "\n",
    "    sent_split = inputs.split(\".\")\n",
    "\n",
    "    if '' in sent_split:\n",
    "        sent_split.remove('')\n",
    "\n",
    "    index_tensor = []\n",
    "\n",
    "    for sent in sent_split:\n",
    "        tokens = basic_tokenizer(sent)\n",
    "        index_tensor.append([word2idx[word] for word in tokens])\n",
    "\n",
    "    return index_tensor, num_encoder_symbols\n",
    "\n",
    "encoder_inputs, num_encoder_symbols = make_data(inputs)\n",
    "decoder_correct, num_decoder_symbols = make_data(outputs)\n",
    "encoder_inputs_length = tf.placeholder(\n",
    "            dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\n",
    "keep_prob_placeholder = tf.placeholder(tf.float32, shape=[], name='keep_prob')\n",
    "batch_size = tf.shape(encoder_inputs)[0]\n",
    "\n",
    "\n",
    "enc_fw_cell = rnn.GRUCell(hidden_units)\n",
    "enc_bw_cell = rnn.GRUCell(hidden_units)\n",
    "\n",
    "# fw_initial_state = enc_fw_cell.zero_state(batch_size, tf.float32)\n",
    "# bw_initial_state = enc_bw_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Initialize encoder_embeddings to have variance=1.\n",
    "sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "initializer = tf.random_uniform_initializer(-sqrt3, sqrt3, dtype=tf.float32)\n",
    "\n",
    "encoder_embeddings = tf.get_variable(name='embedding', initializer=initializer, dtype=tf.float32,\n",
    "                                     shape=[num_encoder_symbols, embedding_size])\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    params=encoder_embeddings, ids=encoder_inputs)\n",
    "\n",
    "input_layer = Dense(hidden_units, dtype=tf.float32, name='input_projection')\n",
    "\n",
    "encoder_inputs_embedded = input_layer(encoder_inputs_embedded)\n",
    "\n",
    "encoder_outputs, encoder_output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=enc_fw_cell,\n",
    "    cell_bw=enc_bw_cell,\n",
    "    inputs=encoder_inputs_embedded,\n",
    "    # initial_state_fw=fw_initial_state,\n",
    "    # initial_state_bw=bw_initial_state,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=encoder_inputs_length)\n",
    "\n",
    "# Building attention mechanism: Default Bahdanau\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_units, memory=encoder_outputs,\n",
    "    memory_sequence_length=encoder_output_states, )\n",
    "\n",
    "decoder_cell = rnn.GRUCell(hidden_units)\n",
    "initial_state = [state for state in encoder_output_states]\n",
    "decoder_initial_state = tuple(initial_state)\n",
    "\n",
    "\n",
    "attn_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_size=hidden_units,\n",
    "                                                name=\"attention_init\")\n",
    "wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    cell=decoder_cell,\n",
    "    attention_mechanism=attention_mechanism,\n",
    "    attention_layer_size=hidden_units,\n",
    "    initial_cell_state=encoder_output_states[-1],\n",
    "    name='Attention_Wrapper')\n",
    "\n",
    "decoder_embeddings = tf.get_variable(name='embedding',\n",
    "                                          shape=[num_decoder_symbols, embedding_size],\n",
    "                                          initializer=initializer, dtype=tf.float32)\n",
    "\n",
    "output_layer = Dense(num_decoder_symbols, name='output_projection')\n",
    "\n",
    "\n",
    "def decoding(mode):\n",
    "    if mode == 'train':\n",
    "\n",
    "        decoder_inputs = tf.placeholder(\n",
    "            dtype=tf.int32, shape=(None, None), name='decoder_inputs')\n",
    "        decoder_inputs_length = tf.placeholder(\n",
    "            dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "\n",
    "        decoder_start_token = tf.ones(\n",
    "            shape=[batch_size, 1], dtype=tf.int32) * start_token\n",
    "        decoder_end_token = tf.ones(\n",
    "            shape=[batch_size, 1], dtype=tf.int32) * end_token\n",
    "\n",
    "        decoder_inputs_train = tf.concat([decoder_start_token,\n",
    "                                          decoder_inputs], axis=1)\n",
    "\n",
    "        decoder_inputs_length_train = decoder_inputs_length + 1\n",
    "\n",
    "        decoder_targets_train = tf.concat([decoder_inputs,\n",
    "                                           decoder_end_token], axis=1)\n",
    "\n",
    "        decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "            params=decoder_embeddings, ids=decoder_inputs_train)\n",
    "\n",
    "        decoder_inputs_embedded = input_layer(decoder_inputs_embedded)\n",
    "\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_inputs_embedded,\n",
    "                                                 sequence_length=decoder_inputs_length_train,\n",
    "                                                 time_major=False,\n",
    "                                                 name='training_helper')\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=wrapped_decoder_cell,\n",
    "                                                helper=training_helper,\n",
    "                                                initial_state=decoder_initial_state,\n",
    "                                                output_layer=output_layer)\n",
    "\n",
    "        max_decoder_length = tf.reduce_max(decoder_inputs_length_train)\n",
    "\n",
    "        decoder_outputs_train, decoder_last_state_train,\\\n",
    "        decoder_outputs_length_train = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder=training_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_decoder_length)\n",
    "\n",
    "        decoder_logits_train = tf.identity(decoder_outputs_train.rnn_output)\n",
    "\n",
    "        masks = tf.sequence_mask(lengths=decoder_inputs_length_train,\n",
    "                                 maxlen=max_decoder_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits=decoder_logits_train,\n",
    "                                          targets=decoder_targets_train,\n",
    "                                          weights=masks,\n",
    "                                          average_across_timesteps=True,\n",
    "                                          average_across_batch=True, )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    elif mode == 'decode':\n",
    "\n",
    "        start_tokens = tf.ones([batch_size, ], tf.int32) * start_token\n",
    "        end_tokens = end_token\n",
    "\n",
    "        def embed_and_input_proj(inputs):\n",
    "            return input_layer(tf.nn.embedding_lookup(decoder_embeddings, inputs))\n",
    "\n",
    "        decoding_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n",
    "                                                                   end_token=end_tokens,\n",
    "                                                                   embedding=embed_and_input_proj)\n",
    "        \n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=wrapped_decoder_cell,\n",
    "                                                            helper=decoding_helper,\n",
    "                                                            initial_state=decoder_initial_state,\n",
    "                                                            output_layer=output_layer)\n",
    "        decoder_outputs_decode, decoder_last_state_decode, \\\n",
    "            decoder_outputs_length_decode = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                decoder=inference_decoder,\n",
    "                                                output_time_major=False,\n",
    "                                                # impute_finished=True,\t# error occurs\n",
    "                                                maximum_iterations=max_decode_step)\n",
    "\n",
    "        decoder_pred_decode = tf.expand_dims(decoder_outputs_decode.sample_id, -1)\n",
    "\n",
    "        return decoder_pred_decode\n",
    "\n",
    "\n",
    "def check_feeds(encoder_inputs, encoder_inputs_length,\n",
    "                decoder_inputs, decoder_inputs_length, decode):\n",
    "\n",
    "    input_batch_size = encoder_inputs.shape[0]\n",
    "    input_feed = dict()\n",
    "    input_feed[encoder_inputs.name] = encoder_inputs\n",
    "    input_feed[encoder_inputs_length.name] = encoder_inputs_length\n",
    "\n",
    "    if not decode:\n",
    "        input_feed[decoder_inputs.name] = decoder_inputs\n",
    "        input_feed[decoder_inputs_length.name] = decoder_inputs_length\n",
    "\n",
    "    return input_feed\n",
    "\n",
    "\n",
    "def predict(sess, encoder_inputs, encoder_inputs_length):\n",
    "    decoder_pred_decode = decoding('decode')\n",
    "    input_feed = check_feeds(encoder_inputs, encoder_inputs_length,\n",
    "                             decoder_inputs=None, decoder_inputs_length=None, decode=True)\n",
    "\n",
    "    input_feed[keep_prob_placeholder.name] = 1.0\n",
    "\n",
    "    output_feed = [decoder_pred_decode]\n",
    "    outputs = sess.run(output_feed, input_feed)\n",
    "\n",
    "    return outputs[0]\n",
    "\n",
    "sequence_loss = decoding('train')\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(num_encoder_symbols):\n",
    "        l, _ = sess.run([loss, train])\n",
    "        result = predict(sess, encoder_inputs_embedded, encoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
